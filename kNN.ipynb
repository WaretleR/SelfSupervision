{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "from timesformer_pytorch import TimeSformer\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dressed-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucfPath = 'D:/Files/Datasets/UCF-101'\n",
    "ucfSplitNumber = 1\n",
    "#ucfPath = 'UCF-101'\n",
    "modelPath = 'model_1'\n",
    "framesPerVideo = 8\n",
    "maxVideoPerClass = 5\n",
    "maxClasses = 98\n",
    "valPerClass = 3\n",
    "embeddingsSize = 256\n",
    "batchSize = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "turned-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTimeSformer(TimeSformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        num_frames,\n",
    "        num_classes,\n",
    "        image_width = 320,\n",
    "        image_height = 240,\n",
    "        patch_size = 16,\n",
    "        channels = 3,\n",
    "        depth = 12,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__(dim = dim, \n",
    "                         num_frames = num_frames, \n",
    "                         num_classes = num_classes, \n",
    "                         image_width = image_width, \n",
    "                         image_height = image_height,\n",
    "                         patch_size = patch_size,\n",
    "                         channels = channels, \n",
    "                         depth = depth,\n",
    "                         heads = heads,\n",
    "                         dim_head = dim_head,\n",
    "                         attn_dropout = attn_dropout,\n",
    "                         ff_dropout = ff_dropout)\n",
    "        self.to_out = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(dim),\n",
    "            torch.nn.Linear(dim, num_classes),\n",
    "            #torch.nn.Softmax(num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) > 1:\n",
    "        modelPath = sys.argv[1]\n",
    "        embeddingsSize = int(sys.argv[2])\n",
    "\n",
    "    print(\"%d %d %d %d %d\" % (tsf_dim, tsf_patch_size, tsf_depth, tsf_heads, tsf_dim_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excited-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:625: UserWarning: Couldn't retrieve source code for container of type AdvancedTimeSformer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrative-upper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AdvancedTimeSformer(\n",
       "    (to_patch_embedding): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (pos_emb): Embedding(2401, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "              (1): GEGLU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "              (1): GEGLU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "              (1): GEGLU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_out): Sequential(\n",
       "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "native-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage():\n",
    "    def __init__(self, ucfDataPath, framesPerVideo, ucfSplitNumber = 1, maxVideoPerClass = None, maxClasses = None):\n",
    "        \n",
    "        ucfFullSize = 13320\n",
    "        self.trainLabelsNames = {}\n",
    "        lastTrainIndex = 0\n",
    "        lastTestIndex = 0\n",
    "        \n",
    "        self.trainDict = defaultdict(list)\n",
    "        self.testDict = defaultdict(list)\n",
    "        if maxClasses is None:\n",
    "            self.trainData = np.zeros((ucfFullSize - 101 * valPerClass, framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "            self.trainLabels = np.zeros((ucfFullSize - 101 * valPerClass), dtype=int)\n",
    "            self.testData = np.zeros((101 * valPerClass, framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "            self.testLabels = np.zeros((101 * valPerClass), dtype=int)\n",
    "        else:\n",
    "            if maxVideoPerClass is None:\n",
    "                numberOfVideos = 0\n",
    "                for k, classFolderName in enumerate(sorted(os.listdir(ucfPath))):\n",
    "                    if k >= maxClasses:\n",
    "                        break\n",
    "                    numberOfVideos += len([name for name in os.listdir(os.path.join(ucfPath, classFolderName)) if os.path.isfile(os.path.join(ucfPath, classFolderName, name))])\n",
    "\n",
    "                print('Number of training and validation videos: %d' % numberOfVideos)\n",
    "                self.trainData = np.zeros((numberOfVideos - valPerClass * maxClasses, framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "                self.trainLabels = np.zeros((numberOfVideos - valPerClass * maxClasses), dtype=int)\n",
    "                self.testData = np.zeros((maxClasses * valPerClass, framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "                self.testLabels = np.zeros((maxClasses * valPerClass), dtype=int)\n",
    "            else:\n",
    "                self.trainData = np.zeros((maxClasses * (maxVideoPerClass - valPerClass), framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "                self.trainLabels = np.zeros((maxClasses * (maxVideoPerClass - valPerClass)), dtype=int)\n",
    "                self.testData = np.zeros((maxClasses * valPerClass, framesPerVideo, 3, 240, 320), dtype=np.uint8)\n",
    "                self.testLabels = np.zeros((maxClasses * valPerClass), dtype=int)                \n",
    "        \n",
    "        for k, classFolderName in enumerate(sorted(os.listdir(ucfPath))):\n",
    "            if maxClasses is not None and k >= maxClasses:\n",
    "                break\n",
    "            \n",
    "            print('Process class ' + classFolderName)\n",
    "            self.trainLabelsNames[classFolderName] = k\n",
    "            for i, videoName in enumerate(sorted(os.listdir(os.path.join(ucfPath, classFolderName)))):\n",
    "                if maxVideoPerClass is not None and i >= maxVideoPerClass:\n",
    "                    break\n",
    "\n",
    "                if i < valPerClass:\n",
    "                    self.testLabels[lastTestIndex] = k\n",
    "                    self.testDict[k].append(lastTestIndex)\n",
    "                else:\n",
    "                    self.trainLabels[lastTrainIndex] = k\n",
    "                    self.trainDict[k].append(lastTrainIndex)\n",
    "\n",
    "                count = 0\n",
    "                video = cv2.VideoCapture(os.path.join(ucfPath, classFolderName, videoName))\n",
    "                numberOfFrames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                for j in range(framesPerVideo):\n",
    "                    video.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
    "                    success, image = video.read()\n",
    "                    if success:\n",
    "                        if image.shape != (240, 320, 3):\n",
    "                            image = cv2.resize(image, (320, 240))\n",
    "                        if i < valPerClass:\n",
    "                            self.testData[lastTestIndex][j] = np.swapaxes(\n",
    "                                                np.swapaxes(image, \n",
    "                                                    0, 2),\n",
    "                                                1, 2)\n",
    "                        else:\n",
    "                            self.trainData[lastTrainIndex][j] = np.swapaxes(\n",
    "                                                np.swapaxes(image, \n",
    "                                                    0, 2),\n",
    "                                                1, 2)\n",
    "                    count += numberOfFrames // framesPerVideo\n",
    "                    \n",
    "                if i < valPerClass:\n",
    "                    lastTestIndex += 1\n",
    "                else:\n",
    "                    lastTrainIndex += 1\n",
    "            \n",
    "        assert lastTrainIndex == self.trainData.shape[0], \"Error in train data length\"\n",
    "        assert lastTestIndex == self.testData.shape[0], \"Error in test data length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identified-visibility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training and validation videos: 12910\n",
      "Process class ApplyEyeMakeup\n",
      "Process class ApplyLipstick\n",
      "Process class Archery\n",
      "Process class BabyCrawling\n",
      "Process class BalanceBeam\n",
      "Process class BandMarching\n",
      "Process class BaseballPitch\n",
      "Process class Basketball\n",
      "Process class BasketballDunk\n",
      "Process class BenchPress\n",
      "Process class Biking\n",
      "Process class Billiards\n",
      "Process class BlowDryHair\n",
      "Process class BlowingCandles\n",
      "Process class BodyWeightSquats\n",
      "Process class Bowling\n",
      "Process class BoxingPunchingBag\n",
      "Process class BoxingSpeedBag\n",
      "Process class BreastStroke\n",
      "Process class BrushingTeeth\n",
      "Process class CleanAndJerk\n",
      "Process class CliffDiving\n",
      "Process class CricketBowling\n",
      "Process class CricketShot\n",
      "Process class CuttingInKitchen\n",
      "Process class Diving\n",
      "Process class Drumming\n",
      "Process class Fencing\n",
      "Process class FieldHockeyPenalty\n",
      "Process class FloorGymnastics\n",
      "Process class FrisbeeCatch\n",
      "Process class FrontCrawl\n",
      "Process class GolfSwing\n",
      "Process class Haircut\n",
      "Process class HammerThrow\n",
      "Process class Hammering\n",
      "Process class HandstandPushups\n",
      "Process class HandstandWalking\n",
      "Process class HeadMassage\n",
      "Process class HighJump\n",
      "Process class HorseRace\n",
      "Process class HorseRiding\n",
      "Process class HulaHoop\n",
      "Process class IceDancing\n",
      "Process class JavelinThrow\n",
      "Process class JugglingBalls\n",
      "Process class JumpRope\n",
      "Process class JumpingJack\n",
      "Process class Kayaking\n",
      "Process class Knitting\n",
      "Process class LongJump\n",
      "Process class Lunges\n",
      "Process class MilitaryParade\n",
      "Process class Mixing\n",
      "Process class MoppingFloor\n",
      "Process class Nunchucks\n",
      "Process class ParallelBars\n",
      "Process class PizzaTossing\n",
      "Process class PlayingCello\n",
      "Process class PlayingDaf\n",
      "Process class PlayingDhol\n",
      "Process class PlayingFlute\n",
      "Process class PlayingGuitar\n",
      "Process class PlayingPiano\n",
      "Process class PlayingSitar\n",
      "Process class PlayingTabla\n",
      "Process class PlayingViolin\n",
      "Process class PoleVault\n",
      "Process class PommelHorse\n",
      "Process class PullUps\n",
      "Process class Punch\n",
      "Process class PushUps\n",
      "Process class Rafting\n",
      "Process class RockClimbingIndoor\n",
      "Process class RopeClimbing\n",
      "Process class Rowing\n",
      "Process class SalsaSpin\n",
      "Process class ShavingBeard\n",
      "Process class Shotput\n",
      "Process class SkateBoarding\n",
      "Process class Skiing\n",
      "Process class Skijet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-246ac9e1ed22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataStorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataStorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mucfPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframesPerVideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-428d5df39bbb>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ucfDataPath, framesPerVideo, ucfSplitNumber, maxVideoPerClass, maxClasses)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframesPerVideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                     \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m240\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m320\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataStorage = DataStorage(ucfPath, framesPerVideo, maxClasses=maxClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "imposed-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "disciplinary-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdvancedTimeSformer(\n",
       "  (to_patch_embedding): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (pos_emb): Embedding(2401, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_out): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.module\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "placed-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEmbeddings = np.zeros((dataStorage.trainData.shape[0], embeddingsSize), dtype=np.float)\n",
    "#testEmbeddings = np.zeros((dataStorage.testData.shape[0], embeddingsSize), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "unlikely-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=maxClasses)\n",
    "\n",
    "indices = [i for i in range(len(dataStorage.trainData))]\n",
    "\n",
    "for batchNumber in range(len(dataStorage.trainData) // batchSize):\n",
    "    inputs = torch.tensor([dataStorage.trainData[i] for i in indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]], dtype = torch.float32)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    trainEmbeddings[batchNumber * batchSize : (batchNumber + 1) * batchSize] = outputs\n",
    "\n",
    "kmeans.fit(trainEmbeddings, dataStorage.trainLabels)\n",
    "\n",
    "correctPreds = 0\n",
    "allPreds = 0\n",
    "\n",
    "indices = [i for i in range(len(dataStorage.testData))]\n",
    "\n",
    "for batchNumber in range(len(dataStorage.testData) // batchSize):\n",
    "    inputs = torch.tensor([dataStorage.testData[i] for i in indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]], dtype = torch.float)\n",
    "    labels = [dataStorage.testLabels[i] for i in indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]]\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    preds = kmeans.predict(outputs)\n",
    "    allPreds += len(labels)\n",
    "    correctPreds += len([i for i in range(len(labels)) if preds[i] == labels[i]])\n",
    "\n",
    "print(correctPreds / allPreds)\n",
    "pickle.dump(kmeans, open('KMeans', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
